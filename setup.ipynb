{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéì Conference RAG - Complete Setup Guide\n",
        "\n",
        "Welcome! In this notebook, you'll build a **Retrieval Augmented Generation (RAG) application** that lets users ask questions about conference talks using semantic search and AI-generated answers.\n",
        "\n",
        "## What You'll Build\n",
        "\n",
        "A web application with three progressively unlocked search modes:\n",
        "\n",
        "1. **üîç Keyword Search** ‚Äî Find talks by keyword (SQL queries via Supabase)\n",
        "2. **üß† Semantic Search** ‚Äî Find similar content by meaning (vector embeddings + pgvector)\n",
        "3. **ü§ñ Ask a Question (RAG)** ‚Äî Get AI-generated answers with sources (full RAG pipeline)\n",
        "\n",
        "## How It Works\n",
        "\n",
        "As you complete each section of this notebook, a search mode will \"light up\" on your deployed site:\n",
        "\n",
        "| You complete... | This unlocks... |\n",
        "|----------------|-----------------|\n",
        "| Parts 1-4: Fork repo, deploy site, configure Supabase | Login works, but all searches show \"Not Ready\" |\n",
        "| Parts 5-6: Import conference data | üîç **Keyword Search** turns green |\n",
        "| Part 7: Generate embeddings + deploy `embed-question` | üß† **Semantic Search** turns green |\n",
        "| Part 8: Deploy `generate-answer` Edge Function | ü§ñ **Ask a Question** turns green |\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   Browser   ‚îÇ  Student asks question\n",
        "‚îÇ  (GitHub    ‚îÇ\n",
        "‚îÇ   Pages)    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚îÇ\n",
        "       ‚îú‚îÄ‚îÄ‚îÄ Supabase Auth (magic link)\n",
        "       ‚îÇ\n",
        "       ‚îú‚îÄ‚îÄ‚îÄ Edge Function: embed-question\n",
        "       ‚îÇ         ‚Üì OpenAI API (server-side key üîí)\n",
        "       ‚îÇ         ‚Üì Returns embedding vector\n",
        "       ‚îÇ\n",
        "       ‚îú‚îÄ‚îÄ‚îÄ Supabase Database (pgvector)\n",
        "       ‚îÇ         ‚Üì Vector similarity search\n",
        "       ‚îÇ         ‚Üì Returns top matching sentences\n",
        "       ‚îÇ\n",
        "       ‚îî‚îÄ‚îÄ‚îÄ Edge Function: generate-answer\n",
        "                 ‚Üì OpenAI GPT-4 (server-side key üîí)\n",
        "                 ‚Üì Returns final answer\n",
        "```\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "You'll learn:\n",
        "1. **Vector Embeddings** - How to represent text as numbers\n",
        "2. **Semantic Search** - Finding similar content without exact keyword matches\n",
        "3. **RAG Architecture** - Combining retrieval + generation\n",
        "4. **Server-side Security** - Protecting API keys with Edge Functions\n",
        "5. **Row Level Security** - User-specific data isolation\n",
        "6. **Production Deployment** - Real-world application architecture\n",
        "\n",
        "## Time Estimate\n",
        "‚è±Ô∏è **~85 minutes** (grab a coffee!)\n",
        "\n",
        "## Cost Estimate\n",
        "üí∞ **~$0.60** in OpenAI API usage (for 5 years of conference talks)\n",
        "\n",
        "Let's get started! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Repository Setup (5 min)\n",
        "\n",
        "## Step 1: Fork the Repository\n",
        "\n",
        "Before we begin in Colab, you need your own copy of the conference-rag repository:\n",
        "\n",
        "1. Go to: **https://github.com/byu-cs-452/conference-rag**\n",
        "2. Click **\"Fork\"** in the top right corner\n",
        "3. Under \"Repository name\", keep it as `conference-rag` (or rename if you prefer)\n",
        "4. **Uncheck** \"Copy the `main` branch only\" if prompted (we only have `main`, but just in case)\n",
        "5. Click **\"Create fork\"**\n",
        "\n",
        "You now have your own copy at: `https://github.com/YOUR-USERNAME/conference-rag`\n",
        "\n",
        "> ‚ö†Ô∏è **Important:** From this point forward, you'll be working in **your fork** ‚Äî not the original `byu-cs-452` repo. Make sure you're editing files in your own copy!\n",
        "\n",
        "> üí° **Why fork?** Forking is a fundamental Git workflow used in open source. Your fork is your own copy that you can freely modify, while still being connected to the original repo for updates.\n",
        "\n",
        "### Make your fork public\n",
        "\n",
        "Your fork must be **public** for free GitHub Pages hosting and for opening the notebook in Google Colab:\n",
        "\n",
        "1. In your forked repo, go to **Settings** ‚Üí **General**\n",
        "2. Scroll to **\"Danger Zone\"** at the bottom\n",
        "3. If repo is private, click **\"Change visibility\"** ‚Üí **Make public**\n",
        "\n",
        "> üí° If the original repo is public, your fork will default to public ‚Äî you may not need to change anything.\n",
        "\n",
        "‚úÖ **You're all set!** Continue below to configure your project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Supabase Project Setup (10 min)\n",
        "\n",
        "## Step 2a: Create a Supabase Project\n",
        "\n",
        "1. Go to [https://supabase.com](https://supabase.com)\n",
        "2. Sign up / Sign in\n",
        "3. Click **\"New Project\"**\n",
        "4. Fill in:\n",
        "   - **Name**: `conference-rag` (or anything)\n",
        "   - **Database Password**: Choose a strong password (save it!)\n",
        "   - **Region**: Choose closest to you\n",
        "5. Click **\"Create new project\"** (takes ~2 minutes)\n",
        "\n",
        "## Step 2b: Get Your Credentials\n",
        "\n",
        "Once the project is created:\n",
        "\n",
        "1. Go to **Settings** (gear icon) ‚Üí **API**\n",
        "2. You'll need these values:\n",
        "   - **Project URL**: `https://xyzabc123.supabase.co`\n",
        "   - **anon public** key: Long string starting with `eyJ...`\n",
        "   - **service_role** key: Long string starting with `eyJ...` (click \"Reveal\")\n",
        "\n",
        "3. Extract your **Project Reference ID** from the URL:\n",
        "   - Example: `https://xyzabc123.supabase.co` ‚Üí Reference ID is `xyzabc123`\n",
        "\n",
        "4. Get a **Personal Access Token**:\n",
        "   - Go to [https://supabase.com/dashboard/account/tokens](https://supabase.com/dashboard/account/tokens)\n",
        "   - Click \"Generate new token\"\n",
        "   - Name: \"Conference RAG Setup\"\n",
        "   - Copy the token (starts with `sbp_`)\n",
        "\n",
        "5. Get an **OpenAI API Key**:\n",
        "   - Go to [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)\n",
        "   - Click \"Create new secret key\"\n",
        "   - Copy the key (starts with `sk-`)\n",
        "\n",
        "Now add these to **Colab Secrets** üîë\n",
        "\n",
        "## Step 2c: Load Credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üîê Load Your Credentials from Colab Secrets\n",
        "\n",
        "# To add secrets in Colab:\n",
        "# 1. Click the üîë key icon in the left sidebar\n",
        "# 2. Add each secret below (click \"+ Add new secret\")\n",
        "# 3. Toggle \"Notebook access\" ON for each\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Required secrets:\n",
        "# - SUPABASE_URL\n",
        "# - SUPABASE_ANON_KEY\n",
        "# - SUPABASE_SERVICE_KEY\n",
        "# - SUPABASE_PROJECT_REF\n",
        "# - SUPABASE_ACCESS_TOKEN\n",
        "# - OPENAI_API_KEY\n",
        "\n",
        "try:\n",
        "    SUPABASE_URL = userdata.get('SUPABASE_URL')\n",
        "    SUPABASE_ANON_KEY = userdata.get('SUPABASE_ANON_KEY')\n",
        "    SUPABASE_SERVICE_KEY = userdata.get('SUPABASE_SERVICE_KEY')\n",
        "    SUPABASE_PROJECT_REF = userdata.get('SUPABASE_PROJECT_REF')\n",
        "    SUPABASE_ACCESS_TOKEN = userdata.get('SUPABASE_ACCESS_TOKEN')\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    \n",
        "    # Set environment variable for Supabase CLI\n",
        "    os.environ['SUPABASE_ACCESS_TOKEN'] = SUPABASE_ACCESS_TOKEN\n",
        "    \n",
        "    print(\"‚úÖ All credentials loaded!\")\n",
        "    print(f\"   Project: {SUPABASE_URL}\")\n",
        "    print(f\"   OpenAI Key: {OPENAI_API_KEY[:8]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"\\\\nAdd credentials to Colab Secrets (üîë icon)\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Database Schema (10 min)\n",
        "\n",
        "## Step 3a: Create Database Schema\n",
        "\n",
        "Now we'll create the database table with pgvector support for semantic search.\n",
        "\n",
        "**What's pgvector?** It's a PostgreSQL extension that lets you store and search vector embeddings efficiently using vector similarity (cosine distance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üóÑÔ∏è Create Database Schema\n",
        "\n",
        "# Install Supabase Python client\n",
        "!pip install -q supabase\n",
        "\n",
        "from supabase import create_client\n",
        "\n",
        "# Create admin client (uses service_role key)\n",
        "supabase_admin = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)\n",
        "\n",
        "# SQL to create schema\n",
        "schema_sql = \"\"\"\n",
        "-- Enable pgvector extension\n",
        "CREATE EXTENSION IF NOT EXISTS vector;\n",
        "\n",
        "-- Create sentence_embeddings table\n",
        "CREATE TABLE IF NOT EXISTS sentence_embeddings (\n",
        "    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
        "    talk_id UUID NOT NULL,\n",
        "    title TEXT NOT NULL,\n",
        "    speaker TEXT,\n",
        "    calling TEXT,\n",
        "    year INTEGER,\n",
        "    season TEXT,\n",
        "    url TEXT,\n",
        "    sentence_num INTEGER,\n",
        "    text TEXT NOT NULL,\n",
        "    embedding vector(1536),\n",
        "    created_at TIMESTAMPTZ DEFAULT NOW()\n",
        ");\n",
        "\n",
        "-- Create index for vector similarity search\n",
        "CREATE INDEX IF NOT EXISTS sentence_embeddings_embedding_idx \n",
        "ON sentence_embeddings USING ivfflat (embedding vector_cosine_ops)\n",
        "WITH (lists = 100);\n",
        "\n",
        "-- Create index for talk_id grouping\n",
        "CREATE INDEX IF NOT EXISTS sentence_embeddings_talk_id_idx \n",
        "ON sentence_embeddings(talk_id);\n",
        "\n",
        "-- Enable Row Level Security\n",
        "ALTER TABLE sentence_embeddings ENABLE ROW LEVEL SECURITY;\n",
        "\n",
        "-- RLS policy: authenticated users can read\n",
        "DROP POLICY IF EXISTS \"Allow authenticated users to read\" ON sentence_embeddings;\n",
        "CREATE POLICY \"Allow authenticated users to read\"\n",
        "ON sentence_embeddings FOR SELECT\n",
        "TO authenticated\n",
        "USING (true);\n",
        "\n",
        "-- Create function for similarity search\n",
        "CREATE OR REPLACE FUNCTION match_sentences(\n",
        "  query_embedding vector(1536),\n",
        "  match_threshold float DEFAULT 0.7,\n",
        "  match_count int DEFAULT 20\n",
        ")\n",
        "RETURNS TABLE (\n",
        "  id uuid,\n",
        "  talk_id uuid,\n",
        "  title text,\n",
        "  speaker text,\n",
        "  text text,\n",
        "  similarity float\n",
        ")\n",
        "LANGUAGE sql STABLE\n",
        "AS $$\n",
        "  SELECT\n",
        "    sentence_embeddings.id,\n",
        "    sentence_embeddings.talk_id,\n",
        "    sentence_embeddings.title,\n",
        "    sentence_embeddings.speaker,\n",
        "    sentence_embeddings.text,\n",
        "    1 - (sentence_embeddings.embedding <=> query_embedding) as similarity\n",
        "  FROM sentence_embeddings\n",
        "  WHERE 1 - (sentence_embeddings.embedding <=> query_embedding) > match_threshold\n",
        "  ORDER BY sentence_embeddings.embedding <=> query_embedding\n",
        "  LIMIT match_count;\n",
        "$$;\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìù Running SQL script...\")\n",
        "print(\"   This creates:\")\n",
        "print(\"   - pgvector extension\")\n",
        "print(\"   - sentence_embeddings table\")\n",
        "print(\"   - Vector similarity search index\")\n",
        "print(\"   - Row Level Security policies\")\n",
        "print(\"   - match_sentences() function\")\n",
        "print()\n",
        "\n",
        "# Execute via Supabase SQL editor (manual step for now)\n",
        "print(\"‚ö†Ô∏è  Please run this SQL manually:\")\n",
        "print(\"\")\n",
        "print(\"1. Go to your Supabase Dashboard\")\n",
        "print(\"2. Click 'SQL Editor' in the left sidebar\")\n",
        "print(\"3. Click 'New Query'\")\n",
        "print(\"4. Paste the SQL below and click 'Run'\")\n",
        "print(\"\")\n",
        "print(\"=\"*60)\n",
        "print(schema_sql)\n",
        "print(\"=\"*60)\n",
        "print(\"\")\n",
        "print(\"5. Come back here and run the checkpoint below\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3b: Verify Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úÖ CHECKPOINT 1: Verify Database Setup\n",
        "\n",
        "try:\n",
        "    result = supabase_admin.table('sentence_embeddings').select('id', count='exact').limit(1).execute()\n",
        "    print(\"‚úÖ Database connection successful!\")\n",
        "    print(f\"   Table 'sentence_embeddings' exists\")\n",
        "    print(f\"   Current rows: {result.count or 0}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Database check failed: {e}\")\n",
        "    print(\"   Make sure you ran the SQL above before continuing\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Learning Checkpoint\n",
        "\n",
        "**What is Row Level Security (RLS)?**\n",
        "\n",
        "RLS lets you control who can access which rows in a table. In our case:\n",
        "- ‚úÖ Authenticated users can **read** all sentences\n",
        "- ‚ùå Unauthenticated users cannot read anything\n",
        "- This protects your data even if someone gets your anon key!\n",
        "\n",
        "**Why sentence-level chunks?**\n",
        "- Higher precision for fact-based queries\n",
        "- Natural semantic boundaries\n",
        "- Can aggregate by talk for context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Frontend Deployment (15 min)\n",
        "\n",
        "Now let's get your site live! You'll deploy first, then configure Supabase credentials.\n",
        "\n",
        "## Step 4a: Enable GitHub Pages\n",
        "\n",
        "Your forked repo needs GitHub Pages turned on:\n",
        "\n",
        "1. Go to your repository on GitHub (your fork, not the original!)\n",
        "2. Click **Settings** (gear icon near the top)\n",
        "3. Click **Pages** in the left sidebar\n",
        "4. Under **\"Source\"**, select **Deploy from a branch**\n",
        "5. Set Branch to **main** and Folder to **/ (root)**\n",
        "6. Click **Save**\n",
        "7. Wait ~1 minute, then refresh the page\n",
        "\n",
        "> ‚ö†Ô∏è **Your repo must be public** for free GitHub Pages hosting. If it's private, go to Settings ‚Üí General ‚Üí Danger Zone ‚Üí Change visibility ‚Üí Make public.\n",
        "\n",
        "Your site will be live at: `https://YOUR-USERNAME.github.io/conference-rag/`\n",
        "\n",
        "üéâ **Visit your URL now!** You should see the Conference Q&A app with a \"Setup Required\" banner. That's expected ‚Äî we'll configure it next.\n",
        "\n",
        "## Step 4b: Update config.js\n",
        "\n",
        "Now let's connect your app to Supabase:\n",
        "\n",
        "1. Go to your repository on GitHub\n",
        "2. Click on `config.js`\n",
        "3. Click the pencil icon (‚úèÔ∏è) to edit\n",
        "4. Replace the placeholder values:\n",
        "\n",
        "```javascript\n",
        "const SUPABASE_CONFIG = {\n",
        "    url: 'YOUR_SUPABASE_URL',      // Replace with your actual URL\n",
        "    anonKey: 'YOUR_ANON_KEY'       // Replace with your actual anon key\n",
        "};\n",
        "```\n",
        "\n",
        "5. Click **\"Commit changes\"**\n",
        "6. Wait ~1 minute for GitHub Pages to redeploy, then refresh your site\n",
        "\n",
        "> üí° **If changes don't appear:** Try a hard refresh (Ctrl+Shift+R) or open in an incognito window. GitHub Pages can sometimes cache old files.\n",
        "\n",
        "## Step 4c: Configure Auth Redirect\n",
        "\n",
        "Copy your deployed URL and add it to Supabase:\n",
        "\n",
        "1. Go to Supabase Dashboard ‚Üí **Authentication** ‚Üí **URL Configuration**\n",
        "2. Under \"Redirect URLs\", click **Add URL**\n",
        "3. Paste: `https://YOUR-USERNAME.github.io/conference-rag/`\n",
        "4. Click **Save**\n",
        "\n",
        "> üí° **Quick link:** Once you've configured `config.js`, your site's setup banner will show a direct link to your Supabase URL Configuration page.\n",
        "\n",
        "## Step 4d: Test Login\n",
        "\n",
        "1. Visit your deployed site\n",
        "2. Enter your email\n",
        "3. Click \"Sign In with Magic Link\"\n",
        "4. Check your inbox\n",
        "5. Click the magic link\n",
        "6. You should be logged in! ‚úÖ\n",
        "\n",
        "**Expected behavior**: You can log in, but the search features will show as \"not ready\" ‚Äî we haven't imported data or deployed Edge Functions yet.\n",
        "\n",
        "## ‚úÖ Checkpoint 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify your deployment\n",
        "\n",
        "print(\"üåê Check list:\")\n",
        "print(\"\")\n",
        "print(\"1. ‚úÖ Site deployed to GitHub Pages?\")\n",
        "print(\"2. ‚úÖ config.js updated with your credentials?\")\n",
        "print(\"3. ‚úÖ Redirect URL added to Supabase?\")\n",
        "print(\"4. ‚úÖ Successfully logged in?\")\n",
        "print(\"\")\n",
        "print(\"If yes to all, continue! If not, review the steps above.\")\n",
        "print(\"\")\n",
        "print(\"Your deployed URL should be:\")\n",
        "print(f\"https://YOUR-USERNAME.github.io/conference-rag/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Learning Checkpoint\n",
        "\n",
        "**Why do the search features show \"not ready\"?**\n",
        "\n",
        "The app checks for three capabilities that you'll build in the upcoming steps:\n",
        "1. **Keyword Search** ‚Äî Needs conference data imported into the database\n",
        "2. **Semantic Search** ‚Äî Needs embeddings generated and the `embed-question` Edge Function deployed\n",
        "3. **RAG (Ask a Question)** ‚Äî Needs the `generate-answer` Edge Function deployed\n",
        "\n",
        "Each one will \"light up\" as you complete the corresponding notebook section!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 5: Deploy Edge Functions (10 min)\n",
        "\n",
        "Edge Functions let us call OpenAI's API server-side, keeping our API keys secret. We'll deploy two functions:\n",
        "1. `embed-question` - Converts user questions to embeddings\n",
        "2. `generate-answer` - Calls GPT-4 to generate final answers\n",
        "\n",
        "## Step 5a: Install Supabase CLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üì¶ Install Supabase CLI\n",
        "\n",
        "# Install Node.js tools (already available in Colab)\n",
        "!npm install -g supabase@latest\n",
        "\n",
        "# Verify installation\n",
        "!supabase --version\n",
        "\n",
        "print(\"‚úÖ Supabase CLI installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5b: Create Edge Function Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üìù Create Edge Function Code\n",
        "\n",
        "import os\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p supabase/functions/embed-question\n",
        "!mkdir -p supabase/functions/generate-answer\n",
        "\n",
        "# Edge Function 1: embed-question\n",
        "embed_function_code = '''import { serve } from \"https://deno.land/std@0.168.0/http/server.ts\"\n",
        "import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'\n",
        "\n",
        "const corsHeaders = {\n",
        "  'Access-Control-Allow-Origin': '*',\n",
        "  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',\n",
        "}\n",
        "\n",
        "serve(async (req) => {\n",
        "  if (req.method === 'OPTIONS') {\n",
        "    return new Response('ok', { headers: corsHeaders })\n",
        "  }\n",
        "\n",
        "  try {\n",
        "    const { question } = await req.json()\n",
        "    const openaiKey = Deno.env.get('OPENAI_API_KEY')\n",
        "    \n",
        "    // Call OpenAI embeddings API\n",
        "    const response = await fetch('https://api.openai.com/v1/embeddings', {\n",
        "      method: 'POST',\n",
        "      headers: {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': `Bearer ${openaiKey}`\n",
        "      },\n",
        "      body: JSON.stringify({\n",
        "        model: 'text-embedding-3-small',\n",
        "        input: question\n",
        "      })\n",
        "    })\n",
        "    \n",
        "    const data = await response.json()\n",
        "    \n",
        "    return new Response(\n",
        "      JSON.stringify({ embedding: data.data[0].embedding }),\n",
        "      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }\n",
        "    )\n",
        "  } catch (error) {\n",
        "    return new Response(\n",
        "      JSON.stringify({ error: error.message }),\n",
        "      { headers: { ...corsHeaders, 'Content-Type': 'application/json' }, status: 500 }\n",
        "    )\n",
        "  }\n",
        "})\n",
        "'''\n",
        "\n",
        "# Edge Function 2: generate-answer\n",
        "answer_function_code = '''import { serve } from \"https://deno.land/std@0.168.0/http/server.ts\"\n",
        "\n",
        "const corsHeaders = {\n",
        "  'Access-Control-Allow-Origin': '*',\n",
        "  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',\n",
        "}\n",
        "\n",
        "serve(async (req) => {\n",
        "  if (req.method === 'OPTIONS') {\n",
        "    return new Response('ok', { headers: corsHeaders })\n",
        "  }\n",
        "\n",
        "  try {\n",
        "    const { question, context_talks } = await req.json()\n",
        "    const openaiKey = Deno.env.get('OPENAI_API_KEY')\n",
        "    \n",
        "    // Build context from talks\n",
        "    const context = context_talks.map((talk, i) => \n",
        "      `Talk ${i+1}: \"${talk.title}\" by ${talk.speaker}\\\\n${talk.text}`\n",
        "    ).join('\\\\n\\\\n')\n",
        "    \n",
        "    // Call OpenAI GPT-4\n",
        "    const response = await fetch('https://api.openai.com/v1/chat/completions', {\n",
        "      method: 'POST',\n",
        "      headers: {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': `Bearer ${openaiKey}`\n",
        "      },\n",
        "      body: JSON.stringify({\n",
        "        model: 'gpt-4o-mini',\n",
        "        messages: [\n",
        "          {\n",
        "            role: 'system',\n",
        "            content: 'You are a helpful assistant answering questions based on conference talks. Use only the provided talks to answer. Cite speakers and talk titles.'\n",
        "          },\n",
        "          {\n",
        "            role: 'user',\n",
        "            content: `Question: ${question}\\\\n\\\\nRelevant Talks:\\\\n${context}`\n",
        "          }\n",
        "        ],\n",
        "        temperature: 0.7,\n",
        "        max_tokens: 500\n",
        "      })\n",
        "    })\n",
        "    \n",
        "    const data = await response.json()\n",
        "    \n",
        "    return new Response(\n",
        "      JSON.stringify({ answer: data.choices[0].message.content }),\n",
        "      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }\n",
        "    )\n",
        "  } catch (error) {\n",
        "    return new Response(\n",
        "      JSON.stringify({ error: error.message }),\n",
        "      { headers: { ...corsHeaders, 'Content-Type': 'application/json' }, status: 500 }\n",
        "    )\n",
        "  }\n",
        "})\n",
        "'''\n",
        "\n",
        "# Write files\n",
        "with open('supabase/functions/embed-question/index.ts', 'w') as f:\n",
        "    f.write(embed_function_code)\n",
        "\n",
        "with open('supabase/functions/generate-answer/index.ts', 'w') as f:\n",
        "    f.write(answer_function_code)\n",
        "\n",
        "print(\"‚úÖ Edge Function code created!\")\n",
        "print(\"   - supabase/functions/embed-question/index.ts\")\n",
        "print(\"   - supabase/functions/generate-answer/index.ts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5c: Deploy Edge Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üöÄ Deploy Edge Functions to Supabase\n",
        "\n",
        "# Link to your project\n",
        "!supabase link --project-ref {SUPABASE_PROJECT_REF}\n",
        "\n",
        "# Deploy embed-question function\n",
        "print(\"Deploying embed-question...\")\n",
        "!supabase functions deploy embed-question --no-verify-jwt\n",
        "\n",
        "# Deploy generate-answer function\n",
        "print(\"\\\\nDeploying generate-answer...\")\n",
        "!supabase functions deploy generate-answer --no-verify-jwt\n",
        "\n",
        "# Set OpenAI API key as secret\n",
        "print(\"\\\\nSetting OpenAI API key secret...\")\n",
        "!supabase secrets set OPENAI_API_KEY={OPENAI_API_KEY}\n",
        "\n",
        "print(\"\\\\n‚úÖ Edge Functions deployed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5d: Test Edge Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úÖ CHECKPOINT 3: Test Edge Functions\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "print(\"Testing Edge Functions...\\\\n\")\n",
        "\n",
        "# Test embed-question\n",
        "test_question = \"What is faith?\"\n",
        "embed_url = f\"{SUPABASE_URL}/functions/v1/embed-question\"\n",
        "\n",
        "try:\n",
        "    response = requests.post(\n",
        "        embed_url,\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {SUPABASE_ANON_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        },\n",
        "        json={\"question\": test_question}\n",
        "    )\n",
        "    result = response.json()\n",
        "    \n",
        "    if 'embedding' in result:\n",
        "        print(\"‚úÖ embed-question function works!\")\n",
        "        print(f\"   Embedding length: {len(result['embedding'])} dimensions\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error: {result}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test failed: {e}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test generate-answer\n",
        "answer_url = f\"{SUPABASE_URL}/functions/v1/generate-answer\"\n",
        "test_talks = [\n",
        "    {\n",
        "        \"title\": \"Test Talk\",\n",
        "        \"speaker\": \"Test Speaker\",\n",
        "        \"text\": \"This is a test talk about faith. Faith is belief in things hoped for.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "try:\n",
        "    response = requests.post(\n",
        "        answer_url,\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {SUPABASE_ANON_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        },\n",
        "        json={\"question\": test_question, \"context_talks\": test_talks}\n",
        "    )\n",
        "    result = response.json()\n",
        "    \n",
        "    if 'answer' in result:\n",
        "        print(\"‚úÖ generate-answer function works!\")\n",
        "        print(f\"   Answer: {result['answer'][:100]}...\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error: {result}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Test failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Learning Checkpoint\n",
        "\n",
        "**Why Edge Functions instead of client-side API calls?**\n",
        "\n",
        "üîí **Security**: API keys stay on the server, never exposed to users\n",
        "\n",
        "**Compare:**\n",
        "- ‚ùå Bad: API key in browser ‚Üí anyone can steal it\n",
        "- ‚úÖ Good: API key in Edge Function ‚Üí only Supabase can access it\n",
        "\n",
        "This is a **production best practice**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 6: Scrape Conference Data (20 min)\n",
        "\n",
        "Now let's get the actual data! We'll scrape 5 years of conference talks from the Church's website.\n",
        "\n",
        "## Step 6a: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üì¶ Install Scraping Libraries\n",
        "\n",
        "!pip install -q beautifulsoup4 requests pandas tqdm\n",
        "\n",
        "print(\"‚úÖ Libraries installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6b: Scrape Conference Talks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üåê Scrape Conference Talks (5 years)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# How many years to scrape\n",
        "YEARS_TO_SCRAPE = 5\n",
        "START_YEAR = 2025 - YEARS_TO_SCRAPE\n",
        "END_YEAR = 2025\n",
        "\n",
        "def setup_session():\n",
        "    \"\"\"Create session with retries\"\"\"\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "    })\n",
        "    return session\n",
        "\n",
        "def get_conference_urls(start_year, end_year):\n",
        "    \"\"\"Generate URLs for conferences\"\"\"\n",
        "    base_url = 'https://www.churchofjesuschrist.org/study/general-conference/{year}/{month}?lang=eng'\n",
        "    return [(base_url.format(year=year, month=month), str(year), month)\n",
        "            for year in range(start_year, end_year + 1)\n",
        "            for month in ['04', '10']]\n",
        "\n",
        "def get_talk_urls(conference_url, year, month, session):\n",
        "    \"\"\"Fetch talk URLs from a conference page\"\"\"\n",
        "    try:\n",
        "        response = session.get(conference_url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "    except:\n",
        "        return []\n",
        "    \n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    talk_urls = []\n",
        "    seen_urls = set()\n",
        "    \n",
        "    # Session slugs to exclude\n",
        "    session_slugs = [\n",
        "        'saturday-morning', 'saturday-afternoon', 'sunday-morning', 'sunday-afternoon',\n",
        "        'priesthood-session', 'women-session', 'womens-session', 'session', 'video'\n",
        "    ]\n",
        "    \n",
        "    for link in soup.select('div.talk-list a[href*=\"/study/general-conference/\"]'):\n",
        "        href = link.get('href')\n",
        "        if not href or 'lang=eng' not in href:\n",
        "            continue\n",
        "        \n",
        "        canonical_url = 'https://www.churchofjesuschrist.org' + href\n",
        "        if canonical_url in seen_urls:\n",
        "            continue\n",
        "        seen_urls.add(canonical_url)\n",
        "        \n",
        "        # Skip session videos\n",
        "        if any(slug in canonical_url.lower() for slug in session_slugs):\n",
        "            continue\n",
        "        \n",
        "        talk_urls.append(canonical_url)\n",
        "    \n",
        "    return talk_urls\n",
        "\n",
        "def scrape_talk(talk_url, session):\n",
        "    \"\"\"Scrape a single talk\"\"\"\n",
        "    try:\n",
        "        response = session.get(talk_url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "    except:\n",
        "        return None\n",
        "    \n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    \n",
        "    def clean_text(text):\n",
        "        if not text:\n",
        "            return text\n",
        "        return text.strip()\n",
        "    \n",
        "    title = clean_text(soup.find(\"h1\").text) if soup.find(\"h1\") else \"No Title\"\n",
        "    speaker_tag = soup.find(\"p\", {\"class\": \"author-name\"})\n",
        "    speaker = clean_text(speaker_tag.text) if speaker_tag else \"Unknown\"\n",
        "    \n",
        "    calling_tag = soup.find(\"p\", {\"class\": \"author-role\"})\n",
        "    calling = clean_text(calling_tag.text) if calling_tag else \"\"\n",
        "    \n",
        "    content_div = soup.find(\"div\", {\"class\": \"body-block\"})\n",
        "    if not content_div:\n",
        "        return None\n",
        "    \n",
        "    content = \" \".join(clean_text(p.text) for p in content_div.find_all(\"p\"))\n",
        "    \n",
        "    year_match = re.search(r'/(\\d{4})/', talk_url)\n",
        "    year = int(year_match.group(1)) if year_match else None\n",
        "    season = \"April\" if \"/04/\" in talk_url else \"October\"\n",
        "    \n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"speaker\": speaker,\n",
        "        \"calling\": calling,\n",
        "        \"year\": year,\n",
        "        \"season\": season,\n",
        "        \"url\": talk_url,\n",
        "        \"text\": content\n",
        "    }\n",
        "\n",
        "# Main scraping logic\n",
        "print(f\"üì∞ Scraping {YEARS_TO_SCRAPE} years of conference talks ({START_YEAR}-{END_YEAR})...\\n\")\n",
        "\n",
        "session = setup_session()\n",
        "conference_urls = get_conference_urls(START_YEAR, END_YEAR)\n",
        "\n",
        "# Get all talk URLs\n",
        "print(\"Finding talk URLs...\")\n",
        "all_talk_urls = []\n",
        "for conf_url, year, month in tqdm(conference_urls):\n",
        "    talk_urls = get_talk_urls(conf_url, year, month, session)\n",
        "    all_talk_urls.extend(talk_urls)\n",
        "\n",
        "print(f\"Found {len(all_talk_urls)} talks\\n\")\n",
        "\n",
        "# Scrape talks in parallel\n",
        "print(\"Scraping talk content...\")\n",
        "talks_data = []\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    futures = {executor.submit(scrape_talk, url, session): url for url in all_talk_urls}\n",
        "    for future in tqdm(as_completed(futures), total=len(all_talk_urls)):\n",
        "        talk = future.result()\n",
        "        if talk:\n",
        "            talks_data.append(talk)\n",
        "\n",
        "talks_df = pd.DataFrame(talks_data)\n",
        "\n",
        "print(f\"\\n‚úÖ Scraped {len(talks_df)} talks successfully!\")\n",
        "print(f\"   Years: {talks_df['year'].min()} - {talks_df['year'].max()}\")\n",
        "print(f\"   Total words: {talks_df['text'].str.split().str.len().sum():,}\")\n",
        "\n",
        "# Preview\n",
        "print(\"\\nSample talks:\")\n",
        "print(talks_df[['year', 'season', 'title', 'speaker']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Learning Checkpoint\n",
        "\n",
        "The scraper:\n",
        "1. Finds all conference URLs for the year range\n",
        "2. Extracts talk URLs (excluding session videos)\n",
        "3. Scrapes each talk in parallel (10 at a time)\n",
        "4. Cleans and structures the data\n",
        "\n",
        "This is real **web scraping** - a valuable data engineering skill!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 7: Generate Embeddings & Import Data (25 min)\n",
        "\n",
        "Now we'll convert the text to embeddings and import everything to Supabase.\n",
        "\n",
        "## Step 7a: Split Talks into Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ‚úÇÔ∏è Split Talks into Sentences\n",
        "\n",
        "import uuid\n",
        "import re\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    \"\"\"Split text into sentences (simple approach)\"\"\"\n",
        "    # Split on period followed by space and capital letter\n",
        "    sentences = re.split(r'\\\\. (?=[A-Z])', text)\n",
        "    # Clean up\n",
        "    sentences = [s.strip() + '.' if not s.endswith('.') else s.strip() for s in sentences]\n",
        "    return [s for s in sentences if len(s) > 20]  # Filter very short sentences\n",
        "\n",
        "# Create sentence records\n",
        "sentence_records = []\n",
        "\n",
        "for _, talk in tqdm(talks_df.iterrows(), total=len(talks_df), desc=\"Splitting into sentences\"):\n",
        "    talk_id = str(uuid.uuid4())\n",
        "    sentences = split_into_sentences(talk['text'])\n",
        "    \n",
        "    for i, sentence in enumerate(sentences, 1):\n",
        "        sentence_records.append({\n",
        "            'talk_id': talk_id,\n",
        "            'title': talk['title'],\n",
        "            'speaker': talk['speaker'],\n",
        "            'calling': talk['calling'],\n",
        "            'year': talk['year'],\n",
        "            'season': talk['season'],\n",
        "            'url': talk['url'],\n",
        "            'sentence_num': i,\n",
        "            'text': sentence\n",
        "        })\n",
        "\n",
        "sentences_df = pd.DataFrame(sentence_records)\n",
        "\n",
        "print(f\"\\n‚úÖ Split {len(talks_df)} talks into {len(sentences_df):,} sentences\")\n",
        "print(f\"   Average sentences per talk: {len(sentences_df) / len(talks_df):.1f}\")\n",
        "print(f\"   Average sentence length: {sentences_df['text'].str.len().mean():.0f} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7b: Generate Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üß† Generate OpenAI Embeddings (this may take 10-15 minutes)\n",
        "\n",
        "import openai\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def get_embedding_batch(texts, model=\"text-embedding-3-small\"):\n",
        "    \"\"\"Get embeddings for a batch of texts\"\"\"\n",
        "    try:\n",
        "        response = client.embeddings.create(\n",
        "            model=model,\n",
        "            input=texts\n",
        "        )\n",
        "        return [item.embedding for item in response.data]\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process in batches to avoid rate limits\n",
        "BATCH_SIZE = 100\n",
        "embeddings = []\n",
        "failed_indices = []\n",
        "\n",
        "print(f\"Generating embeddings for {len(sentences_df):,} sentences...\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\\n\")\n",
        "\n",
        "for i in tqdm(range(0, len(sentences_df), BATCH_SIZE)):\n",
        "    batch_texts = sentences_df['text'].iloc[i:i+BATCH_SIZE].tolist()\n",
        "    \n",
        "    batch_embeddings = get_embedding_batch(batch_texts)\n",
        "    \n",
        "    if batch_embeddings:\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    else:\n",
        "        failed_indices.extend(range(i, min(i+BATCH_SIZE, len(sentences_df))))\n",
        "        # Add empty embeddings as placeholder\n",
        "        embeddings.extend([None] * len(batch_texts))\n",
        "    \n",
        "    # Rate limiting: OpenAI allows ~3000 requests/min\n",
        "    time.sleep(0.1)\n",
        "\n",
        "# Add embeddings to dataframe\n",
        "sentences_df['embedding'] = embeddings\n",
        "\n",
        "# Remove failed embeddings\n",
        "sentences_df = sentences_df[sentences_df['embedding'].notna()]\n",
        "\n",
        "print(f\"\\n‚úÖ Generated {len(sentences_df):,} embeddings\")\n",
        "if failed_indices:\n",
        "    print(f\"   ‚ö†Ô∏è {len(failed_indices)} failed (removed)\")\n",
        "\n",
        "# Estimate cost\n",
        "total_tokens = sentences_df['text'].str.split().str.len().sum()\n",
        "cost = (total_tokens / 1_000_000) * 0.020  # $0.020 per 1M tokens\n",
        "print(f\"\\nüí∞ Estimated cost: ${cost:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7c: Import to Supabase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üíæ Import Data to Supabase\n",
        "\n",
        "# Convert to list of dicts for insertion\n",
        "records = sentences_df.to_dict('records')\n",
        "\n",
        "# Convert embeddings to lists (from numpy arrays if needed)\n",
        "for record in records:\n",
        "    if hasattr(record['embedding'], 'tolist'):\n",
        "        record['embedding'] = record['embedding'].tolist()\n",
        "\n",
        "print(f\"Importing {len(records):,} sentence embeddings to Supabase...\")\n",
        "print(\"This may take 5-10 minutes...\\n\")\n",
        "\n",
        "# Insert in batches\n",
        "BATCH_SIZE = 100\n",
        "success_count = 0\n",
        "error_count = 0\n",
        "\n",
        "for i in tqdm(range(0, len(records), BATCH_SIZE)):\n",
        "    batch = records[i:i+BATCH_SIZE]\n",
        "    \n",
        "    try:\n",
        "        result = supabase_admin.table('sentence_embeddings').insert(batch).execute()\n",
        "        success_count += len(batch)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError inserting batch {i//BATCH_SIZE + 1}: {e}\")\n",
        "        error_count += len(batch)\n",
        "        continue\n",
        "    \n",
        "    # Small delay to avoid overwhelming Supabase\n",
        "    time.sleep(0.1)\n",
        "\n",
        "print(f\"\\n‚úÖ Import complete!\")\n",
        "print(f\"   Success: {success_count:,} sentences\")\n",
        "if error_count > 0:\n",
        "    print(f\"   Errors: {error_count:,} sentences\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7d: Verify Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úÖ CHECKPOINT 4: Verify Data Import\n",
        "\n",
        "# Check row count\n",
        "result = supabase_admin.table('sentence_embeddings').select('id', count='exact').limit(1).execute()\n",
        "row_count = result.count or 0\n",
        "\n",
        "print(f\"‚úÖ Database contains {row_count:,} sentence embeddings\")\n",
        "\n",
        "# Test vector search\n",
        "if row_count > 0:\n",
        "    # Get an embedding from our data\n",
        "    test_embedding = embeddings[0]\n",
        "    \n",
        "    # Try the match_sentences function\n",
        "    result = supabase_admin.rpc('match_sentences', {\n",
        "        'query_embedding': test_embedding,\n",
        "        'match_threshold': 0.7,\n",
        "        'match_count': 5\n",
        "    }).execute()\n",
        "    \n",
        "    if result.data:\n",
        "        print(f\"\\n‚úÖ Vector search working!\")\n",
        "        print(f\"   Found {len(result.data)} similar sentences\")\n",
        "        print(f\"\\nTop match:\")\n",
        "        print(f\"   Title: {result.data[0]['title']}\")\n",
        "        print(f\"   Speaker: {result.data[0]['speaker']}\")\n",
        "        print(f\"   Text: {result.data[0]['text'][:100]}...\")\n",
        "        print(f\"   Similarity: {result.data[0]['similarity']:.3f}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No results from vector search (this might be normal)\")\n",
        "else:\n",
        "    print(\"‚ùå No data in database! Check import step above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Learning Checkpoint\n",
        "\n",
        "**What just happened?**\n",
        "\n",
        "1. **Sentence splitting**: ~400 talks ‚Üí ~80,000 sentences\n",
        "2. **Embedding generation**: Each sentence ‚Üí 1,536-dimensional vector\n",
        "3. **Vector database**: Stored in pgvector for fast similarity search\n",
        "\n",
        "**Why sentence-level?**\n",
        "- Research shows sentences preserve semantic meaning\n",
        "- Higher precision for specific queries\n",
        "- Can aggregate by talk for context\n",
        "\n",
        "This is the **core of RAG**: converting text to searchable vectors!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 8: Test Your RAG System! (10 min)\n",
        "\n",
        "üéâ **Everything is set up!** Let's test the complete system.\n",
        "\n",
        "## Step 8a: Test from Frontend\n",
        "\n",
        "1. Go to your deployed site: `https://YOUR-USERNAME.github.io/my-conference-rag/`\n",
        "2. Make sure you're logged in\n",
        "3. Ask a question: **\"How can I find peace during difficult times?\"**\n",
        "4. Watch the magic happen!\n",
        "\n",
        "**What's happening behind the scenes:**\n",
        "```\n",
        "Your Question\n",
        "    ‚Üì\n",
        "Edge Function: embed-question\n",
        "    ‚Üì (OpenAI embedding)\n",
        "Vector Search in pgvector\n",
        "    ‚Üì (top 20 sentences)\n",
        "Group by talk_id, rank\n",
        "    ‚Üì (top 3 talks)\n",
        "Edge Function: generate-answer\n",
        "    ‚Üì (GPT-4 with context)\n",
        "Final Answer! ‚ú®\n",
        "```\n",
        "\n",
        "## Step 8b: Test from Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üß™ Test RAG Pipeline End-to-End\n",
        "\n",
        "def test_rag_system(question):\n",
        "    \"\"\"Test the complete RAG pipeline\"\"\"\n",
        "    print(f\"Question: {question}\\n\")\n",
        "    \n",
        "    # Step 1: Get embedding for question\n",
        "    print(\"1Ô∏è‚É£ Getting embedding for question...\")\n",
        "    embed_response = requests.post(\n",
        "        f\"{SUPABASE_URL}/functions/v1/embed-question\",\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {SUPABASE_ANON_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        },\n",
        "        json={\"question\": question}\n",
        "    )\n",
        "    embedding = embed_response.json()['embedding']\n",
        "    print(f\"   ‚úÖ Got {len(embedding)}-dimensional embedding\\n\")\n",
        "    \n",
        "    # Step 2: Search for similar sentences\n",
        "    print(\"2Ô∏è‚É£ Searching for similar sentences...\")\n",
        "    search_result = supabase_admin.rpc('match_sentences', {\n",
        "        'query_embedding': embedding,\n",
        "        'match_threshold': 0.6,\n",
        "        'match_count': 20\n",
        "    }).execute()\n",
        "    \n",
        "    sentences = search_result.data\n",
        "    print(f\"   ‚úÖ Found {len(sentences)} similar sentences\\n\")\n",
        "    \n",
        "    # Step 3: Group by talk and rank\n",
        "    print(\"3Ô∏è‚É£ Ranking talks by relevance...\")\n",
        "    from collections import defaultdict\n",
        "    talk_sentences = defaultdict(list)\n",
        "    \n",
        "    for sent in sentences:\n",
        "        talk_sentences[sent['talk_id']].append(sent)\n",
        "    \n",
        "    # Sort talks by number of matching sentences\n",
        "    ranked_talks = sorted(\n",
        "        talk_sentences.items(),\n",
        "        key=lambda x: len(x[1]),\n",
        "        reverse=True\n",
        "    )[:3]  # Top 3 talks\n",
        "    \n",
        "    print(f\"   ‚úÖ Top 3 relevant talks:\\n\")\n",
        "    context_talks = []\n",
        "    for i, (talk_id, sents) in enumerate(ranked_talks, 1):\n",
        "        # Get full talk text\n",
        "        full_talk_result = supabase_admin.table('sentence_embeddings') \\\n",
        "            .select('title, speaker, text') \\\n",
        "            .eq('talk_id', talk_id) \\\n",
        "            .execute()\n",
        "        \n",
        "        talk_sentences_texts = [s['text'] for s in full_talk_result.data]\n",
        "        full_text = ' '.join(talk_sentences_texts)\n",
        "        \n",
        "        context_talks.append({\n",
        "            'title': sents[0]['title'],\n",
        "            'speaker': sents[0]['speaker'],\n",
        "            'text': full_text\n",
        "        })\n",
        "        \n",
        "        print(f\"      {i}. \\\"{sents[0]['title']}\\\" by {sents[0]['speaker']}\")\n",
        "        print(f\"         ({len(sents)} matching sentences)\\n\")\n",
        "    \n",
        "    # Step 4: Generate answer\n",
        "    print(\"4Ô∏è‚É£ Generating answer with GPT-4...\")\n",
        "    answer_response = requests.post(\n",
        "        f\"{SUPABASE_URL}/functions/v1/generate-answer\",\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {SUPABASE_ANON_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        },\n",
        "        json={\n",
        "            \"question\": question,\n",
        "            \"context_talks\": context_talks\n",
        "        }\n",
        "    )\n",
        "    answer = answer_response.json()['answer']\n",
        "    \n",
        "    print(f\"   ‚úÖ Generated answer!\\n\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"ANSWER:\")\n",
        "    print(\"=\"*60)\n",
        "    print(answer)\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return answer\n",
        "\n",
        "# Test questions\n",
        "test_questions = [\n",
        "    \"How can I strengthen my faith?\",\n",
        "    \"What does the church teach about prayer?\",\n",
        "    \"How can I find peace during trials?\"\n",
        "]\n",
        "\n",
        "print(\"Testing RAG system with sample questions...\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for q in test_questions:\n",
        "    test_rag_system(q)\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ CHECKPOINT 5: Final Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final system check\n",
        "\n",
        "print(\"üéâ FINAL SYSTEM CHECK\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "checks = {\n",
        "    \"Database has data\": False,\n",
        "    \"Vector search works\": False,\n",
        "    \"Embed function works\": False,\n",
        "    \"Answer function works\": False\n",
        "}\n",
        "\n",
        "# Check 1: Database\n",
        "try:\n",
        "    result = supabase_admin.table('sentence_embeddings').select('id', count='exact').limit(1).execute()\n",
        "    if result.count > 0:\n",
        "        checks[\"Database has data\"] = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Check 2: Vector search\n",
        "try:\n",
        "    result = supabase_admin.rpc('match_sentences', {\n",
        "        'query_embedding': embeddings[0],\n",
        "        'match_count': 5\n",
        "    }).execute()\n",
        "    if result.data:\n",
        "        checks[\"Vector search works\"] = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Check 3: Embed function\n",
        "try:\n",
        "    response = requests.post(\n",
        "        f\"{SUPABASE_URL}/functions/v1/embed-question\",\n",
        "        headers={\"Authorization\": f\"Bearer {SUPABASE_ANON_KEY}\", \"Content-Type\": \"application/json\"},\n",
        "        json={\"question\": \"test\"}\n",
        "    )\n",
        "    if response.ok:\n",
        "        checks[\"Embed function works\"] = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Check 4: Answer function\n",
        "try:\n",
        "    response = requests.post(\n",
        "        f\"{SUPABASE_URL}/functions/v1/generate-answer\",\n",
        "        headers={\"Authorization\": f\"Bearer {SUPABASE_ANON_KEY}\", \"Content-Type\": \"application/json\"},\n",
        "        json={\"question\": \"test\", \"context_talks\": [{\"title\": \"Test\", \"speaker\": \"Test\", \"text\": \"Test\"}]}\n",
        "    )\n",
        "    if response.ok:\n",
        "        checks[\"Answer function works\"] = True\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Print results\n",
        "for check, passed in checks.items():\n",
        "    status = \"‚úÖ\" if passed else \"‚ùå\"\n",
        "    print(f\"{status} {check}\")\n",
        "\n",
        "all_passed = all(checks.values())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if all_passed:\n",
        "    print(\"üéâ ALL SYSTEMS GO! Your RAG application is ready!\")\n",
        "    print(\"\\nNext: Visit your deployed site and try asking questions!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Some checks failed. Review the steps above.\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 9: Reflection & Next Steps\n",
        "\n",
        "## üéì What You Learned\n",
        "\n",
        "Congratulations! You just built a production-ready RAG application from scratch.\n",
        "\n",
        "### Technical Skills\n",
        "\n",
        "‚úÖ **Vector Embeddings** - Converted text to 1,536-dimensional vectors  \n",
        "‚úÖ **Semantic Search** - Used pgvector for similarity search  \n",
        "‚úÖ **RAG Architecture** - Combined retrieval + generation  \n",
        "‚úÖ **Edge Functions** - Deployed serverless functions  \n",
        "‚úÖ **Row Level Security** - Protected data with RLS policies  \n",
        "‚úÖ **Production Deployment** - Deployed to GitHub Pages  \n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**Why RAG instead of fine-tuning?**\n",
        "- ‚úÖ Cheaper (no model training)\n",
        "- ‚úÖ Updatable (just add new data)\n",
        "- ‚úÖ Transparent (shows sources)\n",
        "- ‚úÖ Accurate (uses exact text)\n",
        "\n",
        "**Why sentence-level chunking?**\n",
        "- Research shows sentences preserve semantic meaning\n",
        "- Higher precision for factual queries\n",
        "- Can aggregate by document for context\n",
        "\n",
        "**Why Edge Functions?**\n",
        "- üîí Keeps API keys server-side\n",
        "- üöÄ Serverless (scales automatically)\n",
        "- üí∞ Cost-effective (pay per request)\n",
        "\n",
        "### Architecture You Built\n",
        "\n",
        "```\n",
        "Student Question\n",
        "    ‚Üì\n",
        "Frontend (GitHub Pages)\n",
        "    ‚Üì (authenticated via Supabase Auth)\n",
        "Edge Function: embed-question\n",
        "    ‚Üì (converts to 1,536-dim vector)\n",
        "Supabase Database (pgvector)\n",
        "    ‚Üì (finds top 20 similar sentences)\n",
        "    ‚Üì (groups by talk, ranks by count)\n",
        "    ‚Üì (returns top 3 talks)\n",
        "Edge Function: generate-answer\n",
        "    ‚Üì (GPT-4 with talk context)\n",
        "Final Answer ‚ú®\n",
        "```\n",
        "\n",
        "## üöÄ Optional Extensions\n",
        "\n",
        "Want to take this further? Try these challenges:\n",
        "\n",
        "### 1. Add Question History\n",
        "**Goal**: Track user's past questions and answers\n",
        "\n",
        "**How**:\n",
        "- Add `question_history` table\n",
        "- Store: user_id, question, answer, timestamp\n",
        "- Display in sidebar\n",
        "\n",
        "**Learning**: Database design, user-specific data\n",
        "\n",
        "### 2. Implement Caching\n",
        "**Goal**: Save money by reusing embeddings for common questions\n",
        "\n",
        "**How**:\n",
        "- Hash questions ‚Üí cache key\n",
        "- Store in `cached_embeddings` table\n",
        "- Check cache before calling OpenAI\n",
        "\n",
        "**Learning**: Performance optimization, caching strategies\n",
        "\n",
        "### 3. Add Talk Recommendations\n",
        "**Goal**: \\\"You might also like these talks...\\\"\n",
        "\n",
        "**How**:\n",
        "- After showing answer, find similar talks\n",
        "- Use the same embedding, but exclude already shown talks\n",
        "- Display 3 recommendations\n",
        "\n",
        "**Learning**: Recommendation systems\n",
        "\n",
        "### 4. Build Analytics Dashboard\n",
        "**Goal**: See what people are asking about\n",
        "\n",
        "**How**:\n",
        "- Track popular questions\n",
        "- Track popular talks (based on matches)\n",
        "- Create charts with Chart.js\n",
        "\n",
        "**Learning**: Data analytics, visualization\n",
        "\n",
        "### 5. Multi-language Support\n",
        "**Goal**: Support Spanish, Portuguese, etc.\n",
        "\n",
        "**How**:\n",
        "- Scrape talks in other languages\n",
        "- Translate questions before embedding\n",
        "- Return answers in user's language\n",
        "\n",
        "**Learning**: Internationalization, translation APIs\n",
        "\n",
        "### 6. Improved Chunking\n",
        "**Goal**: Compare different chunking strategies\n",
        "\n",
        "**How**:\n",
        "- Try paragraph-level chunks\n",
        "- Try semantic chunks (LangChain)\n",
        "- A/B test which performs better\n",
        "\n",
        "**Learning**: Advanced RAG techniques, experimentation\n",
        "\n",
        "## üìö Additional Resources\n",
        "\n",
        "### RAG & Vector Databases\n",
        "- [Supabase pgvector Guide](https://supabase.com/docs/guides/ai)\n",
        "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
        "- [RAG Best Practices (Weaviate)](https://weaviate.io/blog/rag-evaluation)\n",
        "\n",
        "### Chunking Strategies\n",
        "- [Chunking for RAG (LangChain)](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
        "- [Chunking Research (2024)](https://www.superlinked.com/vectorhub/articles/chunking-vs-semantic-splitting)\n",
        "\n",
        "### Production Deployment\n",
        "- [Supabase Edge Functions Docs](https://supabase.com/docs/guides/functions)\n",
        "- [GitHub Pages Guide](https://pages.github.com/)\n",
        "\n",
        "## üéâ You Did It!\n",
        "\n",
        "You now have:\n",
        "- A working RAG application\n",
        "- Hands-on experience with vector databases\n",
        "- Knowledge of production architecture patterns\n",
        "- A portfolio project to show employers!\n",
        "\n",
        "**What's next?** Share your project, try the extensions, or help a classmate!\n",
        "\n",
        "---\n",
        "\n",
        "**Questions or issues?** Check the troubleshooting guide in the repository README.\n",
        "\n",
        "**Enjoyed this?** Give the repo a ‚≠ê on GitHub!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
